{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c76b27b-af3e-4b84-89a5-7b4c064c8d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  Combinations  soil  landcover  SLC\n",
      "0            0   soil3 LULC1     3          1    1\n",
      "1            1   soil3 LULC5     3          5    2\n",
      "2            2   soil3 LULC6     3          6    3\n",
      "3            3   soil3 LULC8     3          8    4\n",
      "4            4  soil3 LULC10     3         10    5\n",
      "5            5  soil3 LULC14     3         14    6\n",
      "6            6  soil3 LULC15     3         15    7\n",
      "7            7  soil3 LULC16     3         16    8\n",
      "8            8  soil3 LULC17     3         17    9\n",
      "9            9  soil3 LULC18     3         18   10\n",
      "10          10  soil3 LULC19     3         19   11\n",
      "11          11   soil8 LULC1     8          1   12\n",
      "12          12  soil8 LULC10     8         10   13\n",
      "13          13  soil8 LULC16     8         16   14\n",
      "14          14  soil8 LULC19     8         19   15\n",
      "The indexes of all DataFrames are exactly the same with the same order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shg096/FHIMP/virtual_env/fhimp_venv/lib/python3.10/site-packages/hydrant/models/HYPE.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combination['Main crop cropid'] = 0\n"
     ]
    }
   ],
   "source": [
    "import hydrant.models.HYPE as HP\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# create the GeoClass\n",
    "HRUs = pd.read_csv('./data/gis/HRU.csv')\n",
    "HP.GeoClass(HRUs,\n",
    "            './data/HYPE/HYPE_Geo_Class.txt',\n",
    "            mapping = {'SLC':'comb','landcover':'LULC','soil':'soil'},\n",
    "            commented_lines = '! HYPE GeoClass')\n",
    "\n",
    "# create the GeoData\n",
    "elevation = pd.read_csv('./data/gis/West_stats_elv.csv')\n",
    "HRU_frac = pd.read_csv('./data/gis/HRU_frac.csv')\n",
    "ntopo1 = pd.read_csv('./data/topology/West_mizuRoute_ntopo.csv'); ntopo1 = ntopo1.drop(columns = ['slope_taud', 'lengthdir'])\n",
    "ntopo2 = pd.read_csv('./data/topology/West_mizuRoute_ntopo.csv'); ntopo2 = ntopo2.drop(columns = ['slope_taud', 'lengthdir'])\n",
    "\n",
    "HP.GaoData (ntopo1, elevation, ntopo2, HRU_frac,\n",
    "            df_mappings = {\n",
    "                'df1': {'id': 'COMID', 'rename_dict': {'subid':'subid','maindown':'NextDownID',\\\n",
    "                                                       'area':'unitarea',\\\n",
    "                                                       'latitude':'latitude','longitude':'longitude',\\\n",
    "                                                       'up_area':'uparea'}},\n",
    "                'df2': {'id': 'COMID', 'rename_dict': {'elev_mean':'mean'}},\n",
    "                'df3': {'id': 'COMID', 'rename_dict': {'slope_mean':'slope','rivlen':'length'}},\n",
    "                'df4': {'id': 'COMID', 'rename_dict': {'SLC_':'comb_'}}},\n",
    "            outfile = './data/HYPE/HYPE_Geo_Data.txt')\n",
    "\n",
    "# create the Param file\n",
    "HP.Parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fhimp_venv",
   "language": "python",
   "name": "fhimp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
